{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de39392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 기능 정의\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - self.start_time\n",
    "        print(f\"Elapsed time: {elapsed_time:.5f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb51c76",
   "metadata": {},
   "source": [
    "arr : 파이썬의 **기본** 리스트 **자료구조** \n",
    "\n",
    "tensor : pytorch의 **행렬 연산**을 하기 위한 자료구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
    "tensor = torch.tensor([ [1,2,3], [4,5,6], [7,8,9] ], dtype = torch.float32)\n",
    "\n",
    "print(arr)\n",
    "print(tensor, \"\\n\", tensor.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1068cbb",
   "metadata": {},
   "source": [
    "### 파이썬 List의 대규모 내적 계산 (CPU 사용)\n",
    "\n",
    "VS\n",
    "\n",
    "### Pytorch Tensor의 대규모 내적 계산 (CPU 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37741d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 일반 파이썬 내적 (O^3)\n",
    "def naive_A_AT(A):\n",
    "    n = len(A)\n",
    "    return [\n",
    "        [sum(A[i][k] * A[j][k] for k in range(n)) for j in range(n)]\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "# 300 x 300 크기의 2차원 리스트 (일반 Python 리스트)\n",
    "arr = [[random.random() for _ in range(300)] for _ in range(300)]\n",
    "# 300 x 300 크기의 2차원 tensor, 난수 범위는 평균(μ)=0, 표준편차(σ)=1인 정규 분포(Gaussian Distribution)\n",
    "tensor = torch.randn(300, 300)    \n",
    "\n",
    "# 각각 자기자신 @ 자기자신.T 내적 시작\n",
    "with Timer():\n",
    "    print(\"파이썬 기본 리스트 자료구조의 내적 (CPU)\")\n",
    "    arr_dot = naive_A_AT(arr)\n",
    "\n",
    "# 내부 C/C++ 로 인터페이스 전달 -> CPU 사용하지만 벡터화(vectorization), 멀티스레딩 활용\n",
    "with Timer():\n",
    "    print(\"파이토치 Tensor 타입의 내적 (CPU)\")\n",
    "    tensor_dot = tensor @ tensor.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b061d6",
   "metadata": {},
   "source": [
    "### torch의 행렬 연산을 GPU를 사용한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단, 하드웨어 특성상 행렬 크기가 작으면 CPU가 더 빠름\n",
    "cpu_tensor = torch.randn(1000, 1000).to(\"cpu\")\n",
    "gpu_tensor = torch.randn(1000, 1000).to(\"cuda:0\")\n",
    "\n",
    "with Timer():\n",
    "    print(\"파이토치 Tensor 타입의 내적 (CPU)\")\n",
    "    tensor_dot = cpu_tensor @ cpu_tensor.t()\n",
    "\n",
    "with Timer():\n",
    "    print(\"파이토치 Tensor 타입의 내적 (GPU)\")\n",
    "    tensor_dot = gpu_tensor @ gpu_tensor.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f2398",
   "metadata": {},
   "source": [
    "### tensor 자료구조의 내장 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a96eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(100, 100)\n",
    "\n",
    "print(\"Shape:\", tensor.shape) # 차원 수를 나타냄 Ex) (배치크기, 데이터 수 , 피쳐 수) 처럼\n",
    "print(\"Data type:\", tensor.dtype) # 타입, 기본값은 float.32 !\n",
    "print(\"Device:\", tensor.device) # 현재 Tensor가 상주하고있는 device\n",
    "print(\"Dimensions:\", tensor.ndim) # 텐서의 차원수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f616b24",
   "metadata": {},
   "source": [
    "### Tensor의 차원 관리 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1793e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터의 배치 크기가 10, 배치당 유저수가 100, 유저당 특징 수가 50 이라고 하면 다음과 같은 행렬이 준비됨\n",
    "tensor = torch.randn(10, 100, 50)\n",
    "\n",
    "# .transpose 연산 (dim0, dim1) -> 특정 두 차원을 교환\n",
    "t_tensor = tensor.transpose(0, 2)\n",
    "print(\"0번째 차원과, 2번쨰 차원을 변경 -> \",t_tensor.shape, \"\\n\\n\")\n",
    "\n",
    "# 활용 예시 - CNN ) (N, C, H, W) -> (N, H, W, C)\n",
    "x = torch.randn(8, 3, 224, 224)   # 8장, 채널 3, 224x224\n",
    "x_t = x.transpose(1, 3)          # (8, 224, 224, 3) 로 변경 -> RNN 모델의 입력 형식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .view 함수 -> 메모리 공유\n",
    "# 100명의 유저가 있고, 유저당 50개의 특징이 있는데 이를 10개의 배치로 분할하고싶음\n",
    "arr = [i for i in range(1,1000+1)]\n",
    "tensor = torch.tensor(arr).to('cuda:0')\n",
    "tensor = tensor.view(100,10).to('cuda:0')\n",
    "tensor = tensor.view(10,10,10).to('cuda:0')\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2add09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .reshape 함수 -> 텐서를 복사\n",
    "# 100명의 유저가 있고, 유저당 50개의 특징이 있는데 이를 10개의 배치로 분할하고싶음\n",
    "arr = [i for i in range(1,1000+1)]\n",
    "tensor = torch.tensor(arr).to('cuda:0')\n",
    "tensor = tensor.reshape(100,10).to('cuda:0')\n",
    "tensor = tensor.reshape(10,10,10).to('cuda:0')\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229c9e6",
   "metadata": {},
   "source": [
    "### squeeze 함수\n",
    "\n",
    "\n",
    "-> 쓸때없는 1차원을 삭제해주는 유틸성 함수!\n",
    "\n",
    "\n",
    "-> 매개변수로 원하는 차원 수를 받고, 이 해당 차원이 1일떄만 삭제, 그렇지 않으면 아무 동작 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3중 분류 MLP 모델에서 최종 출력층의 확률값이 다음과 같이 나왔다고 가정하자.\n",
    "logits = torch.FloatTensor([0.25, 0.45, 0.30])\n",
    "# 근데 여기서 레이블을 추출하기 위해 가장 높은 값의 인덱스를 알고싶다.\n",
    "label = torch.argmax(logits)\n",
    "print(f\"1차원 tensor에서 label -> {label}\",\"\\n\")\n",
    "\n",
    "# 일반적으로 모델의 입/출력 형식이 다음과 같다.\n",
    "logits = torch.FloatTensor(\n",
    "    [\n",
    "        [0.25],\n",
    "        [0.45],\n",
    "        [0.30]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"모델 출력 Shape -> {logits.shape}\")\n",
    "\n",
    "# 근데 여기서 레이블을 추출하기 위해 가장 높은 값의 인덱스를 알고싶으면 차원을 제거해야한다.\n",
    "squeezed_logits = logits.squeeze(1)\n",
    "print(squeezed_logits, squeezed_logits.shape)\n",
    "label = torch.argmax(squeezed_logits)\n",
    "print(f\"다차원 텐서에서 label -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82095f9",
   "metadata": {},
   "source": [
    "### Unsqueeze 함수\n",
    "\n",
    "\n",
    "-> 쓸때없는 1차원을 추가해주는 유틸성 함수!\n",
    "\n",
    "-> 매개변수로 1차원을 추가하고싶은 곳을 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사람의 특징을 가지고있고, 이 사람의 키와 몸무게로 성별을 예측한다고 하자 (이진분류)\n",
    "# 나는 특징 행렬값을 다음과 같이 들고 있다.\n",
    "\n",
    "feature = torch.tensor([180,80])\n",
    "print(f\"나의 데이터 ->\",feature, feature.shape)\n",
    "\n",
    "# MLP 모델은 일반적으로 다음과 같은 형식으로 데이터를 받는다\n",
    "\n",
    "mlp_input = torch.tensor(\n",
    "    [\n",
    "        [180], # 입력층의 첫 번째 뉴런으로 들어가는 값\n",
    "        [80] # 입력층의 두 번째 뉴런으로 들어가는 값\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 그러면 내 데이터에 쓸때없는 1차원 을 추가해야한다. 이때 Unsqueeze를 쓰면 편하다\n",
    "mlp_feature = feature.unsqueeze(1)\n",
    "print(\"언스퀴즈 내 데이터 ->\")\n",
    "print(mlp_feature)\n",
    "print(\"shape -> \",mlp_feature.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bb003",
   "metadata": {},
   "source": [
    "### Mask 행렬 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7cb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_A = torch.tensor([0,0,1,1,0,0])\n",
    "items_B = torch.tensor([0,0,0,0,0,0])\n",
    "\n",
    "mask = (items_A == items_B)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8135cf",
   "metadata": {},
   "source": [
    "### Tensor 자료구조의 Gradient값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(10,5, requires_grad=True)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "y = 2*x + 5\n",
    "y.backward()\n",
    "\n",
    "print(\"x.grad =\", x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5ce954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_grad() 블록 밖에서의 forward 결과 ->  tensor([[ 1.0870,  0.1964,  0.3528,  ..., -0.6517,  0.0755,  0.2146],\n",
      "        [-0.5185, -1.1291,  1.1068,  ...,  0.9242, -0.0219, -0.2693],\n",
      "        [-0.0414,  0.7751, -0.0735,  ..., -0.8630, -0.0411,  1.4447],\n",
      "        ...,\n",
      "        [ 0.4864,  0.8743, -0.1774,  ...,  0.1577, -0.0595, -0.8307],\n",
      "        [ 0.2549,  0.2407, -0.4051,  ..., -0.0610,  0.1141, -1.1097],\n",
      "        [-0.8600,  0.0898, -0.5356,  ..., -0.4654, -0.1645, -0.4457]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "x.grad = tensor([[-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226],\n",
      "        [-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226],\n",
      "        [-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226],\n",
      "        ...,\n",
      "        [-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226],\n",
      "        [-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226],\n",
      "        [-0.5393, -0.5913,  0.0603,  ..., -0.4345, -0.9437,  0.1226]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(20000, 10000)\n",
    "x = torch.randn(10000, 20000, requires_grad=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output_no_grad = model(x)\n",
    "\n",
    "# print(\"no_grad() 블록 안에서의 forward 결과 -> \", output_no_grad)\n",
    "\n",
    "output_grad = model(x)\n",
    "print(\"no_grad() 블록 밖에서의 forward 결과 -> \", output_grad)\n",
    "\n",
    "# 이제 실제 backward 실행\n",
    "# output_grad는 requires_grad=True 이므로 기울기 계산 가능\n",
    "loss = output_grad.sum()  # 임의의 스칼라 예시\n",
    "loss.backward()\n",
    "\n",
    "print()\n",
    "print(\"x.grad =\", x.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025_SCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
